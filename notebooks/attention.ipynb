{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/group/pmc026/nchoong/QuantumTransformer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.getcwd())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:04:29.978037: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 23:04:29.990926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-23 23:04:30.005852: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-23 23:04:30.010397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-23 23:04:30.021576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 23:04:32.428218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(os.cpu_count())\n",
    "tf.config.threading.set_inter_op_parallelism_threads(os.cpu_count())\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings shape: (32, 128, 8)\n"
     ]
    }
   ],
   "source": [
    "# Define the desired shape: (batch_size, sequence_length, embedding_dim)\n",
    "batch_size = 32\n",
    "sequence_length = 128\n",
    "embedding_dim = 8\n",
    "\n",
    "# Generate random word embeddings with the specified shape\n",
    "word_embeddings = np.random.rand(batch_size, sequence_length, embedding_dim).astype(\n",
    "    np.float32\n",
    ")\n",
    "\n",
    "# Print the shape to verify\n",
    "print(f\"Word embeddings shape: {word_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def attention_tf(\n",
    "    query: tf.Tensor,\n",
    "    key: tf.Tensor,\n",
    "    value: tf.Tensor,\n",
    "    mask: tf.Tensor | None = None,\n",
    "    dropout: layers.Dropout | None = None,\n",
    "):\n",
    "    \"\"\"Scaled Dot Product Attention\"\"\"\n",
    "    dim_k = tf.cast(tf.shape(query)[-1], tf.float32)  # type: ignore\n",
    "    # scaled = tf.matmul(query, key, transpose_b=True) / math.sqrt(dim_k)\n",
    "    scaled = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        mask = tf.expand_dims(mask, 1)\n",
    "        scaled = tf.where(mask == 0, -1e9, scaled)\n",
    "    scores = tf.nn.softmax(scaled, axis=-1)\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    # attn = tf.matmul(scores, value)\n",
    "    attn = tf.matmul(scores, value)\n",
    "    return attn, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Dropout\n",
    "\n",
    "\n",
    "def attention_torch(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    mask: Tensor | None = None,\n",
    "    dropout: Dropout | None = None,\n",
    "):\n",
    "    \"\"\"Scaled Dot Product Attention\"\"\"\n",
    "    dim_k = query.size(-1)\n",
    "    # scaled = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_k)\n",
    "    scaled = (query @ key.transpose(-2, -1)) / math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scaled = scaled.masked_fill(mask == 0, -1e9)\n",
    "    scores = scaled.softmax(-1)\n",
    "    if dropout is not None:\n",
    "        scores: Tensor = dropout(scores)\n",
    "    # attn = torch.matmul(scores, value)\n",
    "    attn = scores @ value\n",
    "    return attn, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings shape: (32, 128, 8)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 128\n",
    "embedding_dim = 8\n",
    "num_heads = 2\n",
    "dim_k = embedding_dim // num_heads\n",
    "\n",
    "word_embeddings = np.random.rand(batch_size, sequence_length, embedding_dim).astype(\n",
    "    np.float32\n",
    ")\n",
    "\n",
    "print(f\"Word embeddings shape: {word_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:04:36.094387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-09-23 23:04:36.094877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30828 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 2, 128, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tf = tf.transpose(\n",
    "    tf.reshape(word_embeddings, (batch_size, -1, num_heads, dim_k)),\n",
    "    perm=(0, 2, 1, 3),\n",
    ")\n",
    "input_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 128, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_torch = (\n",
    "    torch.from_numpy(word_embeddings)\n",
    "    .view(batch_size, -1, num_heads, dim_k)\n",
    "    .transpose(1, 2)\n",
    ")\n",
    "input_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_tf, scores_tf = attention_tf(input_tf, input_tf, input_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_torch, scores_torch = attention_torch(input_torch, input_torch, input_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the attention outputs close? True\n",
      "Are the attention scores close? True\n",
      "Max absolute difference in attention outputs: 2.384185791015625e-07\n",
      "Max absolute difference in attention scores: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# Convert PyTorch outputs to NumPy\n",
    "attn_torch_np = attn_torch.detach().cpu().numpy()\n",
    "scores_torch_np = scores_torch.detach().cpu().numpy()\n",
    "\n",
    "# Convert TensorFlow outputs to NumPy\n",
    "attn_tf_np = attn_tf.numpy()\n",
    "scores_tf_np = scores_tf.numpy()\n",
    "\n",
    "# Compare the attention outputs and scores\n",
    "# Check if they are approximately equal (allowing for small floating-point differences)\n",
    "attn_close = np.allclose(attn_tf_np, attn_torch_np, atol=1e-6)\n",
    "scores_close = np.allclose(scores_tf_np, scores_torch_np, atol=1e-6)\n",
    "\n",
    "# Output the comparison results\n",
    "print(f\"Are the attention outputs close? {attn_close}\")\n",
    "print(f\"Are the attention scores close? {scores_close}\")\n",
    "\n",
    "# Alternatively, you can print absolute differences\n",
    "print(\n",
    "    f\"Max absolute difference in attention outputs: {np.max(np.abs(attn_tf_np - attn_torch_np))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Max absolute difference in attention scores: {np.max(np.abs(scores_tf_np - scores_torch_np))}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
