{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import GPUtil\n",
    "from typing import Callable, Iterable\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, device\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = (\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds: Tensor, y: Tensor):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    # round predictions to the closest integer\n",
    "    rounded_preds = preds.sigmoid().round()\n",
    "\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module: nn.Module, N: int):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data: list[tuple[int, str]], tokenizer: Callable[[str], list[str]]):\n",
    "    for label, text in data:\n",
    "        yield tokenizer(text.lower())\n",
    "\n",
    "\n",
    "def text_transform(text: str, vocab: Vocab, tokenizer: Callable[[str], list[str]]):\n",
    "    return vocab([token for token in tokenizer(text.lower())])\n",
    "\n",
    "\n",
    "def label_transform(label: int):\n",
    "    return torch.tensor(1.0 if label == 2 else 0.0, dtype=torch.float)\n",
    "\n",
    "\n",
    "def collate_batch(\n",
    "    batch: Iterable[tuple[int, str]],\n",
    "    vocab: Vocab,\n",
    "    tokenizer: Callable[[str], list[str]],\n",
    "):\n",
    "    pad_idx = vocab[\"<pad>\"]\n",
    "    label_list, text_list = [], []\n",
    "\n",
    "    for label, text in batch:\n",
    "        label_list.append(label_transform(label))\n",
    "        processed_text = torch.tensor(text_transform(text, vocab, tokenizer))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "def batch_sampler(data: list, batch_size: int, tokenizer: Callable[[str], list[str]]):\n",
    "    indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(data)]\n",
    "    random.shuffle(indices)\n",
    "    pooled_indices = []\n",
    "    # create pool of indices with similar lengths\n",
    "    for i in range(0, len(indices), batch_size * 100):\n",
    "        pooled_indices.extend(\n",
    "            sorted(indices[i : i + batch_size * 100], key=lambda x: x[1])\n",
    "        )\n",
    "\n",
    "    pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "    # yield indices for current batch\n",
    "    for i in range(0, len(pooled_indices), batch_size):\n",
    "        yield pooled_indices[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time: float, end_time: float):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    criterion: _Loss,\n",
    "    max_seq_len: int,\n",
    "):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    model.train()\n",
    "    for i, (lab, text) in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.LongTensor(text.T).to(dev)\n",
    "        if inputs.size(1) > max_seq_len:\n",
    "            inputs = inputs[:, :max_seq_len]\n",
    "        model.to(dev)\n",
    "        predictions = model(inputs).squeeze(1)\n",
    "\n",
    "        label = lab.to(dev)\n",
    "        # label = label.unsqueeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        # loss = F.nll_loss(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_acc.append(acc.item())\n",
    "\n",
    "    # print(epoch_loss, epoch_acc, len(dataloader.dataset))\n",
    "\n",
    "    # divide the total loss by the total number of batches per epoch\n",
    "    return np.mean(epoch_loss), np.mean(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module, dataloader: DataLoader, criterion: _Loss, max_seq_len: int\n",
    "):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (lab, text) in enumerate(dataloader):\n",
    "            inputs = torch.LongTensor(text.T).to(dev)\n",
    "            if inputs.size(1) > max_seq_len:\n",
    "                inputs = inputs[:, :max_seq_len]\n",
    "            predictions = model(inputs).squeeze(1)\n",
    "\n",
    "            label = lab.to(dev)\n",
    "            # label = label.unsqueeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            # loss = F.nll_loss(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc.item())\n",
    "\n",
    "    # divide the total loss by the total number of batches per epoch\n",
    "    return np.mean(epoch_loss), np.mean(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout=0.1, max_len=512, device=device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model, device=device)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    mask: Tensor | None = None,\n",
    "    dropout: nn.Dropout | None = None,\n",
    "):\n",
    "    \"\"\"Scaled Dot Product Attention\"\"\"\n",
    "    dim_k = query.size(-1)\n",
    "    # scaled = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_k)\n",
    "    scaled = (query @ key.transpose(-2, -1)) / math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scaled = scaled.masked_fill(mask == 0, -1e9)\n",
    "    scores = scaled.softmax(-1)\n",
    "    if dropout is not None:\n",
    "        scores: Tensor = dropout(scores)\n",
    "    # attn = torch.matmul(scores, value)\n",
    "    attn = scores @ value\n",
    "    return attn, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import templates\n",
    "from pennylane.qnn.torch import TorchLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout=0.1,\n",
    "        mask: Tensor | None = None,\n",
    "        use_bias=False,\n",
    "        n_qubits=4,\n",
    "        n_qlayers=1,\n",
    "        q_device=\"default.qubit\",\n",
    "    ):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mask = mask\n",
    "        # We assume dim_v always equals dim_k\n",
    "        self.dim_k = embed_dim // num_heads  # projection dimensions\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.q_device = q_device\n",
    "\n",
    "        # self.dev = qml.device(q_device, wires=self.n_qubits)\n",
    "        self.dev = qml.device(q_device, wires=self.n_qubits, torch_device=\"cuda\")\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def qlayer(inputs, weights):\n",
    "            templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation=\"Z\")\n",
    "            templates.BasicEntanglerLayers(\n",
    "                weights, wires=range(n_qubits), rotation=qml.RZ\n",
    "            )\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "        self.weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {self.n_qubits})\")\n",
    "\n",
    "        # The quantum layers for the query, key, and value projections\n",
    "        self.k_linear = TorchLayer(qlayer, self.weight_shapes)\n",
    "        self.q_linear = TorchLayer(qlayer, self.weight_shapes)\n",
    "        self.v_linear = TorchLayer(qlayer, self.weight_shapes)\n",
    "        # The quantum layer to combine the heads\n",
    "        self.combine_heads = TorchLayer(qlayer, self.weight_shapes)\n",
    "\n",
    "        self.attn_weights: Tensor | None = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor | None = None\n",
    "    ):\n",
    "        batch_size, seq_len, embed_dim = query.size()\n",
    "        assert (\n",
    "            embed_dim == self.embed_dim\n",
    "        ), f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = [self.k_linear(key[:, t, :]) for t in range(seq_len)]\n",
    "        Q = [self.q_linear(query[:, t, :]) for t in range(seq_len)]\n",
    "        V = [self.v_linear(value[:, t, :]) for t in range(seq_len)]\n",
    "\n",
    "        K = torch.Tensor(pad_sequence(K))\n",
    "        Q = torch.Tensor(pad_sequence(Q))\n",
    "        V = torch.Tensor(pad_sequence(V))\n",
    "\n",
    "        x: Tensor\n",
    "        x, self.attn_weights = attention(Q, K, V, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, embed_dim)\n",
    "\n",
    "        output = [self.combine_heads(x[:, t, :]) for t in range(seq_len)]\n",
    "        output = torch.Tensor(pad_sequence(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Feed-Forward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    The x that is passed to the forward method is a tensor of shape (batch_size, sequence_length, embedding_dimension),\n",
    "    rather than a flattened version of it (with shape (batch_size, sequence_length * embedding_dimension)).\n",
    "    The (same) feed-forward layer applies to the last dimension only (the embedding dimension) for each batch and\n",
    "    for each position in the sequence, hence position-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        n_qubits: int,\n",
    "        n_qlayers=1,\n",
    "        dropout=0.1,\n",
    "        q_device=\"default.qubit\",\n",
    "    ):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = n_qubits\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "        # self.dev = qml.device(q_device, wires=self.n_qubits)\n",
    "        self.dev = qml.device(q_device, wires=self.n_qubits, torch_device=\"cuda\")\n",
    "\n",
    "        def _circuit(inputs, weights):\n",
    "            templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation=\"Z\")\n",
    "            templates.BasicEntanglerLayers(\n",
    "                weights, wires=range(n_qubits), rotation=qml.RZ\n",
    "            )\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "        self.linear_1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.qlayer = qml.QNode(_circuit, self.dev, interface=\"torch\")\n",
    "        self.linear_2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "\n",
    "        self.weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        self.vqc = TorchLayer(self.qlayer, self.weight_shapes)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        x = self.linear_1(x)\n",
    "        X = [self.vqc(x[:, t, :]) for t in range(seq_len)]\n",
    "        x = torch.Tensor(pad_sequence(X))\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ffn_dim: int,\n",
    "        dropout: float = 0.1,\n",
    "        mask: Tensor | None = None,\n",
    "        n_qubits_transformer=0,\n",
    "        n_qubits_ffn=0,\n",
    "        n_qlayers=1,\n",
    "        q_device=\"default.qubit\",\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.n_qubits_transformer = n_qubits_transformer\n",
    "        self.n_qubits_ffn = n_qubits_ffn\n",
    "        self.n_qlayers = n_qlayers\n",
    "\n",
    "        self.attn = MultiHeadedAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            mask=mask,\n",
    "            n_qubits=n_qubits_transformer,\n",
    "            n_qlayers=n_qlayers,\n",
    "            q_device=q_device,\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn = FeedForward(\n",
    "            embed_dim, n_qubits_ffn, n_qlayers, q_device=q_device, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        attn_output = self.attn(x, x, x)\n",
    "        x = self.norm1(attn_output + x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        ff_output: Tensor = self.ffn(x)\n",
    "        x = self.norm2(ff_output + x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        num_classes: int,\n",
    "        vocab_size: int,\n",
    "        ffn_dim=32,\n",
    "        dropout=0.1,\n",
    "        n_qubits_transformer=0,\n",
    "        n_qubits_ffn=0,\n",
    "        n_qlayers=1,\n",
    "        q_device=\"default.qubit\",\n",
    "    ):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = PositionalEncoder(embed_dim)\n",
    "\n",
    "        print(f\"++ There will be {num_blocks} transformer blocks\")\n",
    "\n",
    "        print(\n",
    "            f\"++ Transformer will use {n_qubits_transformer} qubits and {n_qlayers} q layers\"\n",
    "        )\n",
    "\n",
    "        print(f\"The feed-forward head will use {n_qubits_ffn} qubits\")\n",
    "\n",
    "        print(f\"Using quantum device {q_device}\")\n",
    "\n",
    "        self.transformers = get_clones(\n",
    "            Encoder(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                ffn_dim,\n",
    "                n_qubits_transformer=n_qubits_transformer,\n",
    "                n_qubits_ffn=n_qubits_ffn,\n",
    "                n_qlayers=n_qlayers,\n",
    "                q_device=q_device,\n",
    "            ),\n",
    "            num_blocks,\n",
    "        )\n",
    "\n",
    "        self.class_logits = nn.Linear(embed_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        tokens = self.token_embedding(x)\n",
    "        x = self.pos_embedding(tokens)\n",
    "\n",
    "        # # Normalise the data to range [0, 2*pi]\n",
    "        # min_val = x.min()\n",
    "        # max_val = x.max()\n",
    "\n",
    "        # x = (x - min_val) / (max_val - min_val) * (2 * torch.pi)\n",
    "\n",
    "        # print(f\"Minimum value: {x.min()}, Maximum value: {x.max()}\")\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            x = transformer(x)\n",
    "\n",
    "        x = x.mean(dim=1)  # global average pooling, works in 1D\n",
    "        x = self.dropout(x)\n",
    "        x = self.class_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "def main(\n",
    "    max_seq_len=128,\n",
    "    batch_size=32,\n",
    "    total_size=3200,\n",
    "    n_epochs=30,\n",
    "    lr=0.001,\n",
    "    embed_dim=8,\n",
    "    num_heads=4,\n",
    "    num_blocks=6,\n",
    "    num_classes=2,\n",
    "    vocab_size=50000,\n",
    "    ffn_dim=16,\n",
    "    n_qubits_transformer=0,\n",
    "    n_qubits_ffn=0,\n",
    "    n_qlayers=0,\n",
    "    q_device=\"default.qubit\",\n",
    "    dropout_rate=0.1,\n",
    "):\n",
    "\n",
    "    train_iter = IMDB(root=\"./.datatext\", split=\"train\")\n",
    "    test_iter = IMDB(root=\"./.datatext\", split=\"test\")\n",
    "\n",
    "    train_data = to_map_style_dataset(train_iter)\n",
    "    test_data = to_map_style_dataset(test_iter)\n",
    "\n",
    "    size = total_size\n",
    "    # train_data = np.array(train_data)[\n",
    "    #     np.random.choice(len(train_data), size=size, replace=False)\n",
    "    # ].tolist()\n",
    "    test_data = np.array(test_data)[\n",
    "        np.random.choice(len(test_data), size=size, replace=False)\n",
    "    ].tolist()\n",
    "\n",
    "    train_data = [(int(label), text) for label, text in train_data]\n",
    "    test_data = [(int(label), text) for label, text in test_data]\n",
    "\n",
    "    print(\"pos: \", len([label for label, text in train_data if label == 1]))\n",
    "    print(\"neg: \", len([label for label, text in train_data if label == 2]))\n",
    "\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(train_data, tokenizer),\n",
    "        specials=[\"<unk>\", \"<pad>\"],\n",
    "        max_tokens=vocab_size,\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,  # type: ignore\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: collate_batch(batch, vocab, tokenizer),\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,  # type: ignore\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: collate_batch(batch, vocab, tokenizer),\n",
    "    )\n",
    "\n",
    "    model = TextClassifier(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_blocks=num_blocks,\n",
    "        num_classes=num_classes,\n",
    "        vocab_size=vocab_size,\n",
    "        ffn_dim=ffn_dim,\n",
    "        n_qubits_transformer=n_qubits_transformer,\n",
    "        n_qubits_ffn=n_qubits_ffn,\n",
    "        n_qlayers=n_qlayers,\n",
    "        dropout=dropout_rate,\n",
    "        q_device=q_device,\n",
    "    )\n",
    "    print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "\n",
    "    model.to(dev)\n",
    "\n",
    "    optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # logits -> sigmoid -> loss\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    # training loop\n",
    "    best_test_loss = float(\"inf\")\n",
    "    train_loss_list, train_acc_list, test_loss_list, test_acc_list = [], [], [], []\n",
    "    for iepoch in range(n_epochs):\n",
    "        start_time = time()\n",
    "\n",
    "        print(f\"Epoch {iepoch+1}/{n_epochs}\")\n",
    "        train_loss, train_acc = train(\n",
    "            model, train_loader, optimizer, criterion, max_seq_len\n",
    "        )\n",
    "        # GPUtil.showUtilization()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, max_seq_len)\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # if test_loss < best_test_loss:\n",
    "        #     best_test_loss = test_loss\n",
    "        #     torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "        print(f\"Epoch: {iepoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        print(f\"\\tTest Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    return (train_loss_list, train_acc_list, test_loss_list, test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, test_loss, test_acc = main(\n",
    "    n_epochs=40,\n",
    "    embed_dim=8,\n",
    "    n_qubits_transformer=8,\n",
    "    n_qubits_ffn=8,\n",
    "    n_qlayers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# plt.style.use(\"classic\")\n",
    "# plt.style.use(\"dark_background\")\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, \"b-\", label=\"Train Loss\")\n",
    "plt.plot(epochs, test_loss, \"r-\", label=\"Test Loss\")\n",
    "plt.title(\"Train and Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, \"b-\", label=\"Train Accuracy\")\n",
    "plt.plot(epochs, test_acc, \"r-\", label=\"Test Accuracy\")\n",
    "plt.title(\"Train and Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
