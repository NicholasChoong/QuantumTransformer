{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = (\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(dev)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./.cache/datasets\"\n",
    "\n",
    "# datasets = {\n",
    "#     \"imdb\": load_dataset(\"imdb\", cache_dir=cache_dir),\n",
    "#     \"yelp\": load_dataset(\"yelp_polarity\", cache_dir=cache_dir),\n",
    "#     \"amazon\": load_dataset(\"amazon_polarity\", cache_dir=cache_dir),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "cache_folder = \"./.cache/huggingface\"\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=cache_folder,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# bert_model = BertModel.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=cache_folder,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    # Replace single quotes that are not preceded by a backslash\n",
    "    text = re.sub(r\"(?<!\\\\)'\", '\"', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, data_name, sample_size=600_000):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_folder,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    bert_model = BertModel.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_folder,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    data_train_val = data[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    datasets = {\n",
    "        \"train\": data_train_val[\"train\"],\n",
    "        \"val\": data_train_val[\"test\"],\n",
    "        \"test\": data[\"test\"],\n",
    "    }\n",
    "\n",
    "    for dataset_type, dataset in datasets.items():\n",
    "        print(f\"Processing {data_name} {dataset_type} dataset\")\n",
    "\n",
    "        data_labels = dataset[\"label\"]\n",
    "\n",
    "        if \"text\" in dataset.column_names:\n",
    "            data_text = [clean_text(text) for text in dataset[\"text\"]]\n",
    "        else:\n",
    "            data_text = [\n",
    "                clean_text(title).upper() + \": \" + clean_text(content)\n",
    "                for title, content in zip(dataset[\"title\"], dataset[\"content\"])\n",
    "            ]\n",
    "\n",
    "        data = list(zip(data_labels, data_text))\n",
    "\n",
    "        print(f\"Checking {data_name} {dataset_type} dataset size\")\n",
    "        print(f\"Number of samples before: {len(data)}\")\n",
    "        if len(data) > sample_size:\n",
    "            print(f\"Sampling {data_name} {dataset_type} dataset\")\n",
    "            data = random.sample(data, sample_size)\n",
    "        print(f\"Number of samples after: {len(data)}\")\n",
    "\n",
    "        data_labels, data_text = zip(*data)\n",
    "\n",
    "        data_labels = list(data_labels)\n",
    "        data_text = list(data_text)\n",
    "\n",
    "        print(f\"Saving {data_name} {dataset_type} labels\")\n",
    "        labels_file_path = f\"word_labels/{data_name}_{dataset_type}_labels.pkl\"\n",
    "\n",
    "        with open(labels_file_path, \"wb\") as f:\n",
    "            pickle.dump(data_labels, f)\n",
    "\n",
    "        token_file_path = f\"word_tokens/{data_name}_{dataset_type}_tokens.pt\"\n",
    "        batch_size = 1024 * 3\n",
    "\n",
    "        if not os.path.exists(token_file_path):\n",
    "            print(f\"Tokenizing {data_name} {dataset_type} dataset\")\n",
    "            tokens = bert_tokenizer(\n",
    "                data_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "            )\n",
    "\n",
    "            print(f\"Saving {data_name} {dataset_type} word embeddings\")\n",
    "            torch.save(tokens, token_file_path)\n",
    "        else:\n",
    "            tokens = torch.load(token_file_path)\n",
    "\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        print(f\"Computing {data_name} {dataset_type} word embeddings\")\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, input_ids.size(0), batch_size):\n",
    "                batch_input_ids = input_ids[i : i + batch_size].to(dev)\n",
    "                batch_attention_mask = attention_mask[i : i + batch_size].to(dev)\n",
    "                outputs = bert_model(\n",
    "                    batch_input_ids, attention_mask=batch_attention_mask\n",
    "                )\n",
    "                batch_word_embeddings = outputs.last_hidden_state.cpu()\n",
    "\n",
    "                torch.save(\n",
    "                    batch_word_embeddings,\n",
    "                    f\"word_embeddings/{data_name}_{dataset_type}_batch_{i//batch_size}.pt\",\n",
    "                )\n",
    "\n",
    "                del (\n",
    "                    batch_word_embeddings,\n",
    "                    outputs,\n",
    "                    batch_input_ids,\n",
    "                    batch_attention_mask,\n",
    "                )\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name, dataset in datasets.items():\n",
    "#     preprocess(dataset, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(load_dataset(\"imdb\", cache_dir=cache_dir), \"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(load_dataset(\"yelp_polarity\", cache_dir=cache_dir), \"yelp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(load_dataset(\"amazon_polarity\", cache_dir=cache_dir), \"amazon\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
