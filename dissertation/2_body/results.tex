\section{Setup}
\label{sec:setup}
The experiments were conducted within a high-performance computing
environment to facilitate both classical and quantum transformer
model training and evaluation. The following describes the computational setup:

\begin{enumerate}
  \item \textbf{Computational Environment}
        \begin{itemize}
          \item \textbf{Hardware}: All experiments were performed on
                UWAâ€™s \textit{Kaya High-Performance Computing (HPC)} cluster.
                The HPC environment was equipped with NVIDIA GPUs, utilised
                to accelerate the training process for classical models and
                simulate quantum circuits.
          \item \textbf{Quantum Simulators}: For quantum-related
                experiments, we employed quantum simulators available in
                \textit{Pennylane} and \textit{TensorCircuit}. These
                frameworks were used to simulate quantum circuits in the
                absence of physical quantum hardware, allowing for
                experiments involving quantum gate-based computations.
          \item \textbf{Deep Learning Frameworks}: The classical
                transformer models were implemented using \textit{PyTorch}
                and \textit{TensorFlow}, leveraging GPU acceleration where applicable.
        \end{itemize}
\end{enumerate}

\section{Experiments}
\label{sec:experiments}

\section{Results}
\label{sec:results}

\section{Analysis}
\label{sec:analysis}
