This dissertation investigates the integration of quantum variational
circuits (VQCs) into transformer models to explore the potential
advantages of quantum computing in natural language processing (NLP)
tasks. By incorporating both basic and strong VQCs, along with
encoding methods such as amplitude and angle encoding, we compare the
performance of quantum transformers to classical transformers.
Extensive experiments were conducted using datasets like IMDb,
Amazon, and Yelp to evaluate models with and without embedding
layers. The results show that quantum models, particularly those
using amplitude encoding, can outperform classical models in certain
configurations, achieving superior accuracy. Additionally, we have
successfully reduced the training time by leveraging efficient
quantum computing frameworks. This work
demonstrates the viability of quantum machine learning
models and provides insights into their potential applications in
complex NLP tasks. The study also discusses future directions for
improving quantum transformer architectures, including further
optimisations in encoding strategies and circuit designs.
